{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d1ed790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm  # Use tqdm from the notebook module\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21304e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_data(csv_file_path):\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Convert the 'Timestamp' column to datetime\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "    # Create a new column for the date part of the timestamp\n",
    "    df['Date'] = df['Timestamp'].dt.date\n",
    "\n",
    "    # Group by the date and count the number of rows per day\n",
    "    daily_counts = df.groupby('Date').size()\n",
    "\n",
    "    # Check for any days with missing data (less than 288 rows)\n",
    "    missing_days = daily_counts[daily_counts < 288].index.tolist()\n",
    "\n",
    "    return missing_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd3431b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_data_async(date_time, data_set_name, session):\n",
    "    base_url = f\"https://api-open.data.gov.sg/v2/real-time/api/{data_set_name}?date={date_time}\"\n",
    "    all_data = []\n",
    "\n",
    "    while True:\n",
    "        # Asynchronously fetch the data\n",
    "        async with session.get(base_url) as response:\n",
    "            data = await response.json()\n",
    "            readings = data.get('data', {}).get('readings', [])\n",
    "\n",
    "            if readings:\n",
    "                for reading in readings:\n",
    "                    reading_data = reading.get('data', [])\n",
    "                    for entry in reading_data:\n",
    "                        entry['Timestamp'] = reading.get('timestamp')  # Add Timestamp for each entry\n",
    "                        all_data.append(entry)\n",
    "            else:\n",
    "                break  # No readings, exit loop\n",
    "\n",
    "            # Check for paginationToken and update URL for next request\n",
    "            pagination_token = data.get('data', {}).get('paginationToken')\n",
    "            if pagination_token:\n",
    "                base_url = f\"https://api-open.data.gov.sg/v2/real-time/api/{data_set_name}?date={date_time}&paginationToken={pagination_token}\"\n",
    "            else:\n",
    "                break  # No more pages\n",
    "\n",
    "    # Convert the collected data into a DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Pivot table if data is available\n",
    "    if not df.empty:\n",
    "        pivot_table = df.pivot_table(index='Timestamp', columns='stationId', values='value', aggfunc='first')\n",
    "        return pivot_table\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if no data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3852e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_missing_month(data_set_name, year, month):\n",
    "    # Format the CSV file path properly\n",
    "    csv_file_path = f\"../data/data_gov_sg/{data_set_name}_data/{data_set_name}_{year}-{month}.csv\"\n",
    "\n",
    "    # Check for missing data days\n",
    "    missing_days = check_missing_data(csv_file_path)\n",
    "    print(\"Missing data on the following days:\", missing_days)\n",
    "\n",
    "    # Load the existing CSV data\n",
    "    old_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Total number of days with missing data\n",
    "    total_days = len(missing_days)\n",
    "    new_data = []\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        with tqdm(total=total_days) as pbar:\n",
    "            # Iterate over each missing day and attempt to fetch the data\n",
    "            for missing_day in missing_days:\n",
    "                timestamp_str = missing_day.strftime(\"%Y-%m-%d\")\n",
    "                try:\n",
    "                    # Fetch the data asynchronously\n",
    "                    df = await get_data_async(timestamp_str, data_set_name, session)\n",
    "                    if not df.empty:\n",
    "                        new_data.append(df)\n",
    "                    pbar.update(1)  # Update progress bar for each day\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching {timestamp_str}: {e}. Retrying...\")\n",
    "                    await asyncio.sleep(5)  # Wait before retrying\n",
    "                    try:\n",
    "                        df = await get_data_async(timestamp_str, data_set_name, session)\n",
    "                        if not df.empty:\n",
    "                            new_data.append(df)\n",
    "                        pbar.update(1)\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Failed to fetch {timestamp_str}: {e2}\")\n",
    "                        await asyncio.sleep(8)  # Longer delay for retry failure\n",
    "\n",
    "                await asyncio.sleep(2)  # Introduce a 2-second delay between requests\n",
    "\n",
    "        # If new data was successfully fetched, concatenate it with old data and save\n",
    "        if new_data:\n",
    "            # Concatenate old and new data\n",
    "            combined = pd.concat([old_data] + new_data)\n",
    "            \n",
    "            # Remove duplicates based on the 'Timestamp' and 'stationId' columns\n",
    "            combined = combined.drop_duplicates(subset=['Timestamp'])\n",
    "            \n",
    "            # Sort by 'Timestamp' in ascending order\n",
    "            combined = combined.sort_values(by='Timestamp', ascending=True)\n",
    "            \n",
    "            # Save the combined data to CSV\n",
    "            combined.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    print(f\"Finished processing missing data for {year}-{month}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b57a4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_dataset(data_set_name):\n",
    "    folder_path = f\"../data/data_gov_sg/{data_set_name}_data/\"\n",
    "\n",
    "    # Get all CSV files in the folder\n",
    "    try:\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading directory {folder_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # List to hold tasks for concurrent execution\n",
    "    tasks = []\n",
    "\n",
    "    # Extract year and month from each filename and check for missing data\n",
    "    for file in files:\n",
    "        # Regex to strictly match filenames like \"rainfall_2020-01.csv\", \"rainfall_2020-12.csv\"\n",
    "        match = re.match(rf\"{data_set_name}_(\\d{{4}})-(\\d{{2}})\\.csv\", file)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            month = match.group(2)  # Keep the month as a two-digit string\n",
    "            \n",
    "            # Add the task to the list (running check_missing_month concurrently)\n",
    "            tasks.append(check_missing_month(data_set_name, year, month))\n",
    "        else:\n",
    "            print(f\"Skipping file with invalid format: {file}\")\n",
    "    \n",
    "    # Run all tasks concurrently\n",
    "    if tasks:\n",
    "        await asyncio.gather(*tasks)\n",
    "    else:\n",
    "        print(\"No valid files to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20f576b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data on the following days: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3aff62210444a12b1510c97c3d4b18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data on the following days: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cfc73e40a9442bb2d956c856770d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data on the following days: [datetime.date(2020, 6, 8), datetime.date(2020, 6, 11)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddf731c5a884cd782eb1418c6e01461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data on the following days: [datetime.date(2020, 7, 1), datetime.date(2020, 7, 2), datetime.date(2020, 7, 29), datetime.date(2020, 7, 30), datetime.date(2020, 7, 31)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eaf5a8bbe8f4298be78e00637c44fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data on the following days: [datetime.date(2020, 8, 1), datetime.date(2020, 8, 2), datetime.date(2020, 8, 3), datetime.date(2020, 8, 4), datetime.date(2020, 8, 5), datetime.date(2020, 8, 6), datetime.date(2020, 8, 7)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bb47e63c144ef7857384003840b788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data on the following days: [datetime.date(2020, 9, 1)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae501cc67cf4088892c5a3dbd82f738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data on the following days: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c34db02a2c4140bacee298221a2a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data on the following days: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34724d89c6274be59e633c62646282cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data on the following days: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fc0c59c7bf4c40810b2ba4979a8ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing missing data for 2020-04.\n",
      "Finished processing missing data for 2020-05.\n",
      "Finished processing missing data for 2020-10.\n",
      "Finished processing missing data for 2020-11.\n",
      "Finished processing missing data for 2020-12.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Index(['stationId'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12468\\2391617239.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mawait\u001b[0m \u001b[0mcheck_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rainfall\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12468\\4252405611.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(data_set_name)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mSkipping file with invalid format: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Run all tasks concurrently\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No valid files to process\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12468\\1054863914.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(data_set_name, year, month)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m# Concatenate old and new data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mcombined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mold_data\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;31m# Remove duplicates based on the 'Timestamp' and 'stationId' columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mcombined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Timestamp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stationId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m# Sort by 'Timestamp' in ascending order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mcombined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Timestamp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6815\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inplace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6816\u001b[0m         \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ignore_index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6818\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6820\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   6946\u001b[0m         \u001b[1;31m# Otherwise, raise a KeyError, same as if you try to __getitem__ with a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6947\u001b[0m         \u001b[1;31m# key that doesn't exist.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6948\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6949\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6950\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6952\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6953\u001b[0m             \u001b[1;31m# GH#45236 This is faster than get_group_index below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: Index(['stationId'], dtype='object')"
     ]
    }
   ],
   "source": [
    "await check_dataset(\"rainfall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873158e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
