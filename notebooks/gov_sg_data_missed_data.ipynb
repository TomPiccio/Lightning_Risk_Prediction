{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d1ed790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm  # Use tqdm from the notebook module\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import re\n",
    "import calendar\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21304e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_data(csv_file_path, year, month):\n",
    "    # Load the CSV into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    month = int(month)\n",
    "\n",
    "    # Convert the 'Timestamp' column to datetime\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "    # Define date bounds\n",
    "    lower_bound = pd.to_datetime(\"2020-04-20\").date()\n",
    "    upper_bound = pd.to_datetime(\"2025-02-17\").date()\n",
    "\n",
    "    # Generate full date range for the month\n",
    "    _, num_days = calendar.monthrange(year, month)\n",
    "    full_range = pd.date_range(start=f\"{year}-{month:02d}-01\", \n",
    "                               end=f\"{year}-{month:02d}-{num_days}\", \n",
    "                               freq='D').date\n",
    "\n",
    "    # Filter the range within bounds\n",
    "    full_range = [d for d in full_range if lower_bound <= d <= upper_bound]\n",
    "\n",
    "    # Create a new column for the date part of the timestamp\n",
    "    df['Date'] = df['Timestamp'].dt.date\n",
    "\n",
    "    # Group by the date and count the number of rows per day\n",
    "    daily_counts = df.groupby('Date').size()\n",
    "\n",
    "    # Identify days with fewer than 288 entries\n",
    "    partial_days = daily_counts[daily_counts < 288].index.tolist()\n",
    "\n",
    "    # Identify days completely missing\n",
    "    missing_days = sorted(set(full_range) - set(daily_counts.index))\n",
    "\n",
    "    # Combine and return all missing or partial days\n",
    "    return sorted(partial_days + missing_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57c4ffe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.date(2020, 6, 8),\n",
       " datetime.date(2020, 6, 9),\n",
       " datetime.date(2020, 6, 10),\n",
       " datetime.date(2020, 6, 11)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_missing_data(r\"D:\\Documents\\Term 8\\Deep Learning\\Lightning_Risk_Prediction\\data\\data_gov_sg\\rainfall_data\\rainfall_2020-06.csv\", 2020, \"06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd3431b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_data_async(date_time, data_set_name, session):\n",
    "    base_url = f\"https://api-open.data.gov.sg/v2/real-time/api/{data_set_name}?date={date_time}\"\n",
    "    all_data = []\n",
    "\n",
    "    while True:\n",
    "        # Asynchronously fetch the data\n",
    "        async with session.get(base_url) as response:\n",
    "            data = await response.json()\n",
    "            readings = data.get('data', {}).get('readings', [])\n",
    "\n",
    "            if readings:\n",
    "                for reading in readings:\n",
    "                    reading_data = reading.get('data', [])\n",
    "                    for entry in reading_data:\n",
    "                        entry['Timestamp'] = reading.get('timestamp')  # Add Timestamp for each entry\n",
    "                        all_data.append(entry)\n",
    "            else:\n",
    "                break  # No readings, exit loop\n",
    "\n",
    "            # Check for paginationToken and update URL for next request\n",
    "            pagination_token = data.get('data', {}).get('paginationToken')\n",
    "            if pagination_token:\n",
    "                base_url = f\"https://api-open.data.gov.sg/v2/real-time/api/{data_set_name}?date={date_time}&paginationToken={pagination_token}\"\n",
    "            else:\n",
    "                break  # No more pages\n",
    "\n",
    "    # Convert the collected data into a DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Pivot table if data is available\n",
    "    if not df.empty:\n",
    "        pivot_table = df.pivot_table(index='Timestamp', columns='stationId', values='value', aggfunc='first')\n",
    "        return pivot_table\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if no data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3852e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_missing_month(data_set_name, year, month):\n",
    "    # Format the CSV file path properly\n",
    "    csv_file_path = f\"../data/data_gov_sg/{data_set_name}_data/{data_set_name}_{year}-{month}.csv\"\n",
    "\n",
    "    # Check for missing data days\n",
    "    missing_days = check_missing_data(csv_file_path, year, month)\n",
    "\n",
    "    # Load the existing CSV data\n",
    "    old_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Total number of days with missing data\n",
    "    total_days = len(missing_days)\n",
    "    new_data = []\n",
    "\n",
    "    if total_days == 0:\n",
    "        print(f\"Data for {year}-{month}. Already Clean!\")\n",
    "        return None\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        with tqdm(total=total_days) as pbar:\n",
    "            # Iterate over each missing day and attempt to fetch the data\n",
    "            for missing_day in missing_days:\n",
    "                timestamp_str = missing_day.strftime(\"%Y-%m-%d\")\n",
    "                try:\n",
    "                    # Fetch the data asynchronously\n",
    "                    df = await get_data_async(timestamp_str, data_set_name, session)\n",
    "                    if not df.empty:\n",
    "                        new_data.append(df)\n",
    "                    pbar.update(1)  # Update progress bar for each day\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fetching {timestamp_str}: {e}. Retrying...\")\n",
    "                    await asyncio.sleep(5)  # Wait before retrying\n",
    "                    try:\n",
    "                        df = await get_data_async(timestamp_str, data_set_name, session)\n",
    "                        if not df.empty:\n",
    "                            new_data.append(df)\n",
    "                            if(len(df)!=288):\n",
    "                                print(timestamp_str,\"only have\",len(df),\"datapoints\")\n",
    "                        pbar.update(1)\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Failed to fetch {timestamp_str}: {e2}\")\n",
    "                        await asyncio.sleep(8)  # Longer delay for retry failure\n",
    "                        pbar.update(1)\n",
    "                await asyncio.sleep(2)  # Introduce a 2-second delay between requests\n",
    "\n",
    "        # If new data was successfully fetched, concatenate it with old data and save\n",
    "        if new_data:\n",
    "            # Concatenate old and new data\n",
    "            combined = pd.concat([old_data] + new_data)\n",
    "            \n",
    "            # Remove duplicates based on the 'Timestamp' and 'stationId' columns\n",
    "            combined = combined.drop_duplicates(subset=['Timestamp'])\n",
    "            \n",
    "            # Sort by 'Timestamp' in ascending order\n",
    "            combined = combined.sort_values(by='Timestamp', ascending=True)\n",
    "            \n",
    "            # Save the combined data to CSV\n",
    "            combined.to_csv(csv_file_path, index=False)\n",
    "\n",
    "    print(f\"Finished processing missing data for {year}-{month}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b57a4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_dataset(data_set_name):\n",
    "    folder_path = f\"../data/data_gov_sg/{data_set_name}_data/\"\n",
    "\n",
    "    # Get all CSV files in the folder\n",
    "    try:\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading directory {folder_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # List to hold tasks for concurrent execution\n",
    "    tasks = []\n",
    "\n",
    "    # Extract year and month from each filename and check for missing data\n",
    "    for file in files:\n",
    "        # Regex to strictly match filenames like \"rainfall_2020-01.csv\", \"rainfall_2020-12.csv\"\n",
    "        match = re.match(rf\"{data_set_name}_(\\d{{4}})-(\\d{{2}})\\.csv\", file)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            month = match.group(2)  # Keep the month as a two-digit string\n",
    "            \n",
    "            # Add the task to the list (running check_missing_month concurrently)\n",
    "            tasks.append(check_missing_month(data_set_name, year, month))\n",
    "        else:\n",
    "            print(f\"Skipping file with invalid format: {file}\")\n",
    "    \n",
    "    # Run all tasks concurrently\n",
    "    if tasks:\n",
    "        await asyncio.gather(*tasks)\n",
    "    else:\n",
    "        print(\"No valid files to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "20f576b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2020-04. Already Clean!\n",
      "Data for 2020-05. Already Clean!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95807fa8a2ce4aed8eb3366f8e3bf54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2020-07. Already Clean!\n",
      "Data for 2020-08. Already Clean!\n",
      "Data for 2020-09. Already Clean!\n",
      "Data for 2020-10. Already Clean!\n",
      "Data for 2020-11. Already Clean!\n",
      "Data for 2020-12. Already Clean!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d110da17ccd4307b66f5136d107ef82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2021-02. Already Clean!\n",
      "Data for 2021-03. Already Clean!\n",
      "Data for 2021-04. Already Clean!\n",
      "Data for 2021-05. Already Clean!\n",
      "Data for 2021-06. Already Clean!\n",
      "Data for 2021-07. Already Clean!\n",
      "Data for 2021-08. Already Clean!\n",
      "Data for 2021-09. Already Clean!\n",
      "Data for 2021-10. Already Clean!\n",
      "Data for 2021-11. Already Clean!\n",
      "Data for 2021-12. Already Clean!\n",
      "Data for 2022-01. Already Clean!\n",
      "Data for 2022-02. Already Clean!\n",
      "Data for 2022-03. Already Clean!\n",
      "Data for 2022-04. Already Clean!\n",
      "Data for 2022-05. Already Clean!\n",
      "Data for 2022-06. Already Clean!\n",
      "Data for 2022-07. Already Clean!\n",
      "Data for 2022-08. Already Clean!\n",
      "Data for 2022-09. Already Clean!\n",
      "Data for 2022-10. Already Clean!\n",
      "Data for 2022-11. Already Clean!\n",
      "Data for 2022-12. Already Clean!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feda7af1f2af4dbab045f16bd86bd735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for 2023-02. Already Clean!\n",
      "Data for 2023-03. Already Clean!\n",
      "Data for 2023-04. Already Clean!\n",
      "Data for 2023-05. Already Clean!\n",
      "Data for 2023-06. Already Clean!\n",
      "Data for 2023-07. Already Clean!\n",
      "Data for 2023-08. Already Clean!\n",
      "Data for 2023-09. Already Clean!\n",
      "Data for 2023-10. Already Clean!\n",
      "Data for 2023-11. Already Clean!\n",
      "Data for 2023-12. Already Clean!\n",
      "Data for 2024-01. Already Clean!\n",
      "Data for 2024-02. Already Clean!\n",
      "Data for 2024-03. Already Clean!\n",
      "Data for 2024-04. Already Clean!\n",
      "Data for 2024-05. Already Clean!\n",
      "Data for 2024-06. Already Clean!\n",
      "Data for 2024-07. Already Clean!\n",
      "Data for 2024-08. Already Clean!\n",
      "Data for 2024-09. Already Clean!\n",
      "Data for 2024-10. Already Clean!\n",
      "Data for 2024-11. Already Clean!\n",
      "Data for 2024-12. Already Clean!\n",
      "Data for 2025-01. Already Clean!\n",
      "Data for 2025-02. Already Clean!\n",
      "Data for 2025-03. Already Clean!\n",
      "Data for 2025-04. Already Clean!\n",
      "Error fetching 2021-01-02: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Error fetching 2023-01-02: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Error fetching 2020-06-09: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Failed to fetch 2021-01-02: 'NoneType' object has no attribute 'get'\n",
      "Failed to fetch 2023-01-02: 'NoneType' object has no attribute 'get'\n",
      "Failed to fetch 2020-06-09: 'NoneType' object has no attribute 'get'\n",
      "Error fetching 2021-01-03: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Error fetching 2023-01-03: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Error fetching 2020-06-10: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Failed to fetch 2021-01-03: 'NoneType' object has no attribute 'get'\n",
      "Failed to fetch 2023-01-03: 'NoneType' object has no attribute 'get'\n",
      "Failed to fetch 2020-06-10: 'NoneType' object has no attribute 'get'\n",
      "Error fetching 2021-01-04: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Error fetching 2023-01-04: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Finished processing missing data for 2020-06.\n",
      "Failed to fetch 2021-01-04: 'NoneType' object has no attribute 'get'\n",
      "Failed to fetch 2023-01-04: 'NoneType' object has no attribute 'get'\n",
      "Error fetching 2023-01-05: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Finished processing missing data for 2021-01.\n",
      "Failed to fetch 2023-01-05: 'NoneType' object has no attribute 'get'\n",
      "Error fetching 2023-01-06: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Failed to fetch 2023-01-06: 'NoneType' object has no attribute 'get'\n",
      "Error fetching 2023-01-07: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Failed to fetch 2023-01-07: 'NoneType' object has no attribute 'get'\n",
      "Error fetching 2023-01-08: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Failed to fetch 2023-01-08: 'NoneType' object has no attribute 'get'\n",
      "Error fetching 2023-01-09: 'NoneType' object has no attribute 'get'. Retrying...\n",
      "Failed to fetch 2023-01-09: 'NoneType' object has no attribute 'get'\n",
      "Finished processing missing data for 2023-01.\n"
     ]
    }
   ],
   "source": [
    "await check_dataset(\"relative-humidity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873158e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
